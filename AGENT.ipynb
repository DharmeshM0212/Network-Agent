{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ac0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import TypedDict, List, Optional, Literal , Dict\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    step: int\n",
    "    window_features: np.ndarray  # shape (15, 7)\n",
    "    predicted_state: Optional[Literal[\"Good\", \"Moderate\", \"Poor\"]]\n",
    "    prediction_probs: Optional[List[float]]\n",
    "    action: Optional[str]\n",
    "    outcome: Optional[Literal[\"Success\", \"Fail\", \"Pending\"]]\n",
    "    similar_case: Optional[dict]\n",
    "    log: List[str]\n",
    "\n",
    "class SelfRAGMemory:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.metadata = []  # No cap\n",
    "\n",
    "    def add(self, input_window, state, action, outcome):\n",
    "        vec = input_window.flatten().astype('float32')\n",
    "        vec /= np.linalg.norm(vec) + 1e-8  # Normalize\n",
    "        self.index.add(np.expand_dims(vec, axis=0))\n",
    "        self.metadata.append((state, action, outcome))\n",
    "\n",
    "    def retrieve(self, input_window, k=3):\n",
    "        if len(self.metadata) == 0:\n",
    "            return []\n",
    "\n",
    "        query_vec = input_window.flatten().astype('float32')\n",
    "        query_vec /= np.linalg.norm(query_vec) + 1e-8\n",
    "        D, I = self.index.search(np.expand_dims(query_vec, axis=0), k)\n",
    "\n",
    "        results = []\n",
    "        for idx, dist in zip(I[0], D[0]):\n",
    "            if idx < len(self.metadata):\n",
    "                similarity = 1 / (1 + dist)\n",
    "                results.append({\n",
    "                    'state': self.metadata[idx][0],\n",
    "                    'action': self.metadata[idx][1],\n",
    "                    'outcome': self.metadata[idx][2],\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "        return results\n",
    "\n",
    "\n",
    "class GlobalLogMemory:\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "    \n",
    "    def add_log(self, entry: str):\n",
    "        self.logs.append(entry)\n",
    "\n",
    "class LSTMClassifierNode:\n",
    "    def __init__(self, model_path: str, scaler: StandardScaler, class_order: List[str]):\n",
    "        self.model = load_model(model_path)\n",
    "        self.scaler = scaler\n",
    "        self.class_order = class_order\n",
    "\n",
    "    def __call__(self, state: AgentState) -> AgentState:\n",
    "        # Prepare window for LSTM\n",
    "        window = state[\"window_features\"].reshape(1, 15, 7)\n",
    "        probs = self.model.predict(window, verbose=0)\n",
    "        pred_idx = int(np.argmax(probs))\n",
    "        pred_class = self.class_order[pred_idx]\n",
    "\n",
    "        # Update state\n",
    "        state[\"predicted_state\"] = pred_class\n",
    "        state[\"prediction_probs\"] = probs[0].tolist()\n",
    "        state[\"log\"].append(\n",
    "            f\"[Step {state['step']}] Predicted: {pred_class} | Probs: {probs[0]}\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2c3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# ====== CONFIG ======\n",
    "OPENROUTER_API_KEY = \"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\"  # <-- paste your key here\n",
    "OPENROUTER_MODEL = \"openai/gpt-4\"  # you can change to other supported models\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "# ====================\n",
    "\n",
    "LLM_CACHE = {}\n",
    "\n",
    "# Create LLM object using OpenRouter endpoint\n",
    "llm = ChatOpenAI(\n",
    "    model=OPENROUTER_MODEL,\n",
    "    temperature=0.3,\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=OPENROUTER_BASE_URL\n",
    ")\n",
    "\n",
    "@tool\n",
    "def select_action(current_state: str, retrieved_memories: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Decides the best action based on the current state and retrieved memory,\n",
    "    using LLM via OpenRouter.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an AI network optimization agent.\n",
    "\n",
    "    Current State: {state}\n",
    "    Retrieved Memory:\n",
    "    {memory_items}\n",
    "\n",
    "    Based on the above, what is the best action to improve the state?\n",
    "    Respond with a short action like: \"Lower Modulation to QPSK + Max FEC\" or \"Increase Transmit Power\".\n",
    "    \"\"\")\n",
    "\n",
    "    # Format retrieved memory text\n",
    "    memory_text = \"\"\n",
    "    for i, mem in enumerate(retrieved_memories):\n",
    "        memory_text += (\n",
    "            f\"{i+1}. State: {mem['state']}, Action: {mem['action']}, \"\n",
    "            f\"Outcome: {mem['outcome']} (Similarity: {mem['similarity']:.2f})\\n\"\n",
    "        )\n",
    "    \n",
    "    # Cache key for identical queries\n",
    "    cache_key = f\"{current_state}::{memory_text}\"\n",
    "\n",
    "    if cache_key in LLM_CACHE:\n",
    "        return LLM_CACHE[cache_key]\n",
    "    \n",
    "    # Call the model\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"state\": current_state, \"memory_items\": memory_text})\n",
    "    action = response.content.strip()\n",
    "    \n",
    "    # Store in cache\n",
    "    LLM_CACHE[cache_key] = action\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff00d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def execute_action(current_window: np.ndarray, action: str, simulation_mode: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulates or applies the effect of an action on the signal feature window.\n",
    "\n",
    "    Args:\n",
    "        current_window: np.ndarray shape (15, 7) - the current sliding window of features\n",
    "        action: str - action chosen by the LLM\n",
    "        simulation_mode: bool - if True, apply effect to entire window; else only last row.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: modified feature window after action\n",
    "    \"\"\"\n",
    "    action_effects = {\n",
    "        \"Lower Modulation to QPSK + Max FEC\": np.array([+0.5, -0.2, -0.3, 0.0, -0.1, +0.2, 0.0]),\n",
    "        \"Increase Transmit Power\":             np.array([+0.7, -0.1, -0.2, 0.0, +0.2, +0.1, 0.0]),\n",
    "        \"Switch Frequency Band\":               np.array([-0.2, +0.1, 0.0, +0.3, -0.4, -0.1, +0.2]),\n",
    "        \"Apply Beamforming\":                   np.array([+0.6, -0.3, -0.2, 0.0, +0.1, +0.4, 0.0]),\n",
    "        \"Use Relay Node\":                      np.array([+0.4, -0.1, -0.1, 0.0, 0.0, 0.0, 0.0]),\n",
    "    }\n",
    "\n",
    "    modified_window = current_window.copy()\n",
    "\n",
    "    for known_action, delta in action_effects.items():\n",
    "        if known_action.lower() in action.lower():\n",
    "            if simulation_mode:\n",
    "                # Apply delta to ALL rows (full simulated trend shift)\n",
    "                modified_window = modified_window + delta\n",
    "            else:\n",
    "                # Apply delta only to the most recent row\n",
    "                modified_window[-1, :] = modified_window[-1, :] + delta\n",
    "            break\n",
    "\n",
    "    return modified_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af23568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "class OutcomeEvaluatorNode:\n",
    "    \"\"\"\n",
    "    Re-evaluates the updated window with the LSTM and decides Success/Fail.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, class_order, delta_thresh: float = 0.10, simulation_mode: bool = True):\n",
    "        self.model = model\n",
    "        self.class_order = class_order\n",
    "        self.delta_thresh = delta_thresh\n",
    "        self.simulation_mode = simulation_mode\n",
    "\n",
    "    def __call__(self, state: \"AgentState\", modified_window: np.ndarray) -> \"AgentState\":\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          - state[\"window_features\"]: (15, 7) standardized features\n",
    "          - modified_window: (15, 7) after ActionExecutor\n",
    "        \"\"\"\n",
    "        prev_probs = np.array(state[\"prediction_probs\"], dtype=float) if state[\"prediction_probs\"] else None\n",
    "        good_idx = self.class_order.index(\"Good\")\n",
    "\n",
    "        # If in real-hardware mode, only replace last row in existing window\n",
    "        if not self.simulation_mode:\n",
    "            new_window = state[\"window_features\"].copy()\n",
    "            new_window[-1, :] = modified_window[-1, :]\n",
    "        else:\n",
    "            # In simulation, the full modified window is already simulated\n",
    "            new_window = modified_window.copy()\n",
    "\n",
    "        # Pass through LSTM\n",
    "        new_probs = self.model.predict(new_window[np.newaxis, :, :], verbose=0)[0]\n",
    "        new_pred_idx = int(np.argmax(new_probs))\n",
    "        new_pred_label = self.class_order[new_pred_idx]\n",
    "\n",
    "        # Decide outcome\n",
    "        if new_pred_label == \"Good\":\n",
    "            outcome = \"Success\"\n",
    "            reason = \"Class flipped to Good.\"\n",
    "        else:\n",
    "            if prev_probs is not None:\n",
    "                gain = float(new_probs[good_idx] - prev_probs[good_idx])\n",
    "                if gain >= self.delta_thresh:\n",
    "                    outcome = \"Success\"\n",
    "                    reason = f\"Good prob improved by +{gain:.2f} (≥ {self.delta_thresh:.2f}).\"\n",
    "                else:\n",
    "                    outcome = \"Fail\"\n",
    "                    reason = f\"Good prob gain +{gain:.2f} (< {self.delta_thresh:.2f}).\"\n",
    "            else:\n",
    "                outcome = \"Fail\"\n",
    "                reason = \"No prior baseline to compare; still not Good.\"\n",
    "\n",
    "        # Update state\n",
    "        state[\"window_features\"] = new_window\n",
    "        state[\"predicted_state\"] = new_pred_label\n",
    "        state[\"prediction_probs\"] = new_probs.tolist()\n",
    "        state[\"outcome\"] = outcome\n",
    "\n",
    "        state[\"log\"].append(\n",
    "            f\"[Step {state['step']}] OutcomeEval → NewState: {new_pred_label} | \"\n",
    "            f\"Probs: {np.round(new_probs, 3).tolist()} | Outcome: {outcome} ({reason})\"\n",
    "        )\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce6d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import TypedDict, List, Optional, Literal, Dict\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import faiss\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "OPENROUTER_API_KEY = \"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\"\n",
    "#OPENROUTER_MODEL = \"openai/gpt-4\"\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# LLM init: cheaper model + max_tokens\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-4o-mini\",   # or \"qwen/qwen2.5-7b-instruct\", \"meta-llama/llama-3.1-8b-instruct\"\n",
    "    temperature=0.3,\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=OPENROUTER_BASE_URL,\n",
    "    max_tokens=128                 # hard cap\n",
    ")\n",
    "\n",
    "LLM_CACHE = {}\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    step: int\n",
    "    window_features: np.ndarray       # shape (15, 7)\n",
    "    predicted_state: Optional[Literal[\"Good\", \"Moderate\", \"Poor\"]]\n",
    "    prediction_probs: Optional[List[float]]\n",
    "    action: Optional[str]\n",
    "    outcome: Optional[Literal[\"Success\", \"Fail\", \"Pending\"]]\n",
    "    similar_case: Optional[List[dict]]\n",
    "    log: List[str]\n",
    "\n",
    "class SelfRAGMemory:\n",
    "    \"\"\"Stores numeric cases for FAISS retrieval.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.metadata = []  # (state, action, outcome)\n",
    "\n",
    "    def add(self, input_window, state, action, outcome):\n",
    "        vec = input_window.flatten().astype('float32')\n",
    "        vec /= np.linalg.norm(vec) + 1e-8\n",
    "        self.index.add(np.expand_dims(vec, axis=0))\n",
    "        self.metadata.append((state, action, outcome))\n",
    "\n",
    "    def retrieve(self, input_window, k=3):\n",
    "        if len(self.metadata) == 0:\n",
    "            return []\n",
    "        query_vec = input_window.flatten().astype('float32')\n",
    "        query_vec /= np.linalg.norm(query_vec) + 1e-8\n",
    "        D, I = self.index.search(np.expand_dims(query_vec, axis=0), k)\n",
    "\n",
    "        results = []\n",
    "        for idx, dist in zip(I[0], D[0]):\n",
    "            if idx < len(self.metadata):\n",
    "                similarity = 1 / (1 + dist)\n",
    "                results.append({\n",
    "                    'state': self.metadata[idx][0],\n",
    "                    'action': self.metadata[idx][1],\n",
    "                    'outcome': self.metadata[idx][2],\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "        return results\n",
    "\n",
    "class GlobalLogMemory:\n",
    "    \"\"\"Keeps human-readable logs of the whole run.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "    \n",
    "    def add_log(self, entry: str):\n",
    "        self.logs.append(entry)\n",
    "\n",
    "    def get_full_log(self):\n",
    "        return \"\\n\".join(self.logs)\n",
    "\n",
    "class LSTMClassifierNode:\n",
    "    def __init__(self, model_path: str, scaler: StandardScaler, class_order: List[str]):\n",
    "        self.model = load_model(model_path)\n",
    "        self.scaler = scaler\n",
    "        self.class_order = class_order\n",
    "\n",
    "    def __call__(self, state: AgentState, global_log: GlobalLogMemory) -> AgentState:\n",
    "        window = state[\"window_features\"].reshape(1, 15, 7)\n",
    "        probs = self.model.predict(window, verbose=0)\n",
    "        pred_idx = int(np.argmax(probs))\n",
    "        pred_class = self.class_order[pred_idx]\n",
    "\n",
    "        state[\"predicted_state\"] = pred_class\n",
    "        state[\"prediction_probs\"] = probs[0].tolist()\n",
    "        log_entry = f\"[Step {state['step']}] Prediction: {pred_class} | Probs: {np.round(probs[0], 3)}\"\n",
    "        state[\"log\"].append(log_entry)\n",
    "        global_log.add_log(log_entry)\n",
    "        return state\n",
    "\n",
    "@tool\n",
    "def select_action(current_state: str, retrieved_memories: List[Dict]) -> str:\n",
    "    \"\"\"Uses LLM to choose the best action.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an AI network optimization agent.\n",
    "\n",
    "    Current State: {state}\n",
    "    Retrieved Memory:\n",
    "    {memory_items}\n",
    "\n",
    "    Based on the above, suggest the best action to improve the state.\n",
    "    Respond briefly, e.g., \"Lower Modulation to QPSK + Max FEC\".\n",
    "    \"\"\")\n",
    "\n",
    "    memory_text = \"\\n\".join(\n",
    "        f\"{i+1}. State: {m['state']}, Action: {m['action']}, Outcome: {m['outcome']} (Sim: {m['similarity']:.2f})\"\n",
    "        for i, m in enumerate(retrieved_memories)\n",
    "    )\n",
    "\n",
    "    cache_key = f\"{current_state}::{memory_text}\"\n",
    "    if cache_key in LLM_CACHE:\n",
    "        return LLM_CACHE[cache_key]\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"state\": current_state, \"memory_items\": memory_text})\n",
    "    action = response.content.strip()\n",
    "    LLM_CACHE[cache_key] = action\n",
    "    return action\n",
    "\n",
    "@tool\n",
    "def execute_action(current_window: np.ndarray, action: str, simulation_mode: bool = True) -> np.ndarray:\n",
    "    \"\"\"Simulates the effect of an action on the feature window.\"\"\"\n",
    "    action_effects = {\n",
    "        \"Lower Modulation to QPSK + Max FEC\": np.array([+0.5, -0.2, -0.3, 0.0, -0.1, +0.2, 0.0]),\n",
    "        \"Increase Transmit Power\":             np.array([+0.7, -0.1, -0.2, 0.0, +0.2, +0.1, 0.0]),\n",
    "        \"Switch Frequency Band\":               np.array([-0.2, +0.1, 0.0, +0.3, -0.4, -0.1, +0.2]),\n",
    "        \"Apply Beamforming\":                   np.array([+0.6, -0.3, -0.2, 0.0, +0.1, +0.4, 0.0]),\n",
    "        \"Use Relay Node\":                      np.array([+0.4, -0.1, -0.1, 0.0, 0.0, 0.0, 0.0]),\n",
    "    }\n",
    "    modified_window = current_window.copy()\n",
    "    for known_action, delta in action_effects.items():\n",
    "        if known_action.lower() in action.lower():\n",
    "            if simulation_mode:\n",
    "                modified_window += delta  # full-window modification\n",
    "            else:\n",
    "                modified_window[-1, :] += delta  # last-row only\n",
    "            break\n",
    "    return modified_window\n",
    "\n",
    "class OutcomeEvaluatorNode:\n",
    "    def __init__(self, model, class_order, delta_thresh: float = 0.10, simulation_mode: bool = True):\n",
    "        self.model = model\n",
    "        self.class_order = class_order\n",
    "        self.delta_thresh = delta_thresh\n",
    "        self.simulation_mode = simulation_mode\n",
    "\n",
    "    def __call__(self, state: AgentState, modified_window: np.ndarray,\n",
    "                 selfrag: SelfRAGMemory, global_log: GlobalLogMemory) -> AgentState:\n",
    "        prev_probs = np.array(state[\"prediction_probs\"], dtype=float)\n",
    "        good_idx = self.class_order.index(\"Good\")\n",
    "\n",
    "        if not self.simulation_mode:\n",
    "            new_window = state[\"window_features\"].copy()\n",
    "            new_window[-1, :] = modified_window[-1, :]\n",
    "        else:\n",
    "            new_window = modified_window.copy()\n",
    "\n",
    "        new_probs = self.model.predict(new_window[np.newaxis, :, :], verbose=0)[0]\n",
    "        new_pred_idx = int(np.argmax(new_probs))\n",
    "        new_pred_label = self.class_order[new_pred_idx]\n",
    "\n",
    "        if new_pred_label == \"Good\":\n",
    "            outcome, reason = \"Success\", \"Class flipped to Good.\"\n",
    "        else:\n",
    "            gain = float(new_probs[good_idx] - prev_probs[good_idx])\n",
    "            if gain >= self.delta_thresh:\n",
    "                outcome, reason = \"Success\", f\"Good prob improved by +{gain:.2f}.\"\n",
    "            else:\n",
    "                outcome, reason = \"Fail\", f\"Good prob gain +{gain:.2f} (< {self.delta_thresh}).\"\n",
    "\n",
    "        state[\"window_features\"] = new_window\n",
    "        state[\"predicted_state\"] = new_pred_label\n",
    "        state[\"prediction_probs\"] = new_probs.tolist()\n",
    "        state[\"outcome\"] = outcome\n",
    "\n",
    "        log_entry = f\"[Step {state['step']}] Outcome: {outcome} | NewState: {new_pred_label} | Probs: {np.round(new_probs, 3)} ({reason})\"\n",
    "        state[\"log\"].append(log_entry)\n",
    "        global_log.add_log(log_entry)\n",
    "\n",
    "        # Store into retrieval memory\n",
    "        selfrag.add(new_window, new_pred_label, state[\"action\"], outcome)\n",
    "        return state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c1b5b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ITER 1] Pred=Good | Action=Keep current settings | Outcome=Success\n",
      "\n",
      "[ITER 2] Pred=Moderate | Action=Increase Transmit Power | Outcome=Fail\n",
      "\n",
      "[ITER 3] Pred=Moderate | Action=\"Lower Modulation to QPSK + Max FEC.\" | Outcome=Fail\n",
      "\n",
      "[ITER 4] Pred=Good | Action=Keep current settings | Outcome=Success\n",
      "\n",
      "[ITER 5] Pred=Moderate | Action=Change Frequency Band. | Outcome=Fail\n",
      "\n",
      "[ITER 6] Pred=Good | Action=Keep current settings | Outcome=Success\n",
      "\n",
      "[ITER 7] Pred=Good | Action=Keep current settings | Outcome=Success\n",
      "\n",
      "[ITER 8] Pred=Good | Action=Keep current settings | Outcome=Success\n",
      "\n",
      "=== Global Log (tail) ===\n",
      "[Step 1] Prediction: Good | Probs: [1. 0. 0.]\n",
      "[Step 1] Good → skipping RAG/LLM.\n",
      "[Step 1] Good window → no action; marked Success.\n",
      "[Step 2] Prediction: Moderate | Probs: [0.    0.998 0.002]\n",
      "[Step 2] Retrieved 0 similar cases.\n",
      "[Step 2] No memory yet → fallback action: Increase Transmit Power\n",
      "[Step 2] Executed action.\n",
      "[Step 2] Outcome: Fail | NewState: Moderate | Probs: [0. 1. 0.] (Good prob gain +0.00 (< 0.1).)\n",
      "[Step 3] Prediction: Poor | Probs: [0.    0.113 0.887]\n",
      "[Step 3] Retrieved 3 similar cases.\n",
      "[Step 3] LLM selected action: \"Lower Modulation to QPSK + Max FEC.\"\n",
      "[Step 3] Executed action.\n",
      "[Step 3] Outcome: Fail | NewState: Moderate | Probs: [0.    0.999 0.001] (Good prob gain +0.00 (< 0.1).)\n",
      "[Step 4] Prediction: Good | Probs: [0.996 0.004 0.   ]\n",
      "[Step 4] Good → skipping RAG/LLM.\n",
      "[Step 4] Good window → no action; marked Success.\n",
      "[Step 5] Prediction: Moderate | Probs: [0.001 0.999 0.   ]\n",
      "[Step 5] Retrieved 3 similar cases.\n",
      "[Step 5] LLM selected action: Change Frequency Band.\n",
      "[Step 5] Executed action.\n",
      "[Step 5] Outcome: Fail | NewState: Moderate | Probs: [0.001 0.999 0.   ] (Good prob gain +0.00 (< 0.1).)\n",
      "[Step 6] Prediction: Good | Probs: [1. 0. 0.]\n",
      "[Step 6] Good → skipping RAG/LLM.\n",
      "[Step 6] Good window → no action; marked Success.\n",
      "[Step 7] Prediction: Good | Probs: [0.999 0.001 0.   ]\n",
      "[Step 7] Good → skipping RAG/LLM.\n",
      "[Step 7] Good window → no action; marked Success.\n",
      "[Step 8] Prediction: Good | Probs: [1. 0. 0.]\n",
      "[Step 8] Good → skipping RAG/LLM.\n",
      "[Step 8] Good window → no action; marked Success.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DATASET_PATH = r\"C:\\Users\\DHARMESH M\\Documents\\Projects\\SIG-COG\\Dataset\\features_dataset.csv\"  # <-- your CSV\n",
    "FEATURE_COLS = ['SNR_dB','BER','PacketLoss_pct','Jitter_ms','Throughput_Mbps','SNR_trend','Jitter_spike']\n",
    "WINDOW_SIZE = 15\n",
    "ITERATIONS = 8                 # how many windows to run\n",
    "START_MODE = \"random\"      # \"sequential\" or \"random\"\n",
    "SIMULATION_MODE = True         # full-window modification (we’re simulating)\n",
    "CLASS_ORDER = [\"Good\", \"Moderate\", \"Poor\"]\n",
    "\n",
    "# ---- shared memories and nodes ----\n",
    "rag_memory = SelfRAGMemory(dim=WINDOW_SIZE * len(FEATURE_COLS))\n",
    "global_log = GlobalLogMemory()\n",
    "\n",
    "# LSTM node — use your real paths/objects\n",
    "# NOTE: If your trained model expects scaled inputs, ensure the CSV is the scaled version\n",
    "scaler = StandardScaler()  # present for interface; we’re not calling transform here\n",
    "classifier_node = LSTMClassifierNode(\n",
    "    model_path=r\"C:\\Users\\DHARMESH M\\Documents\\Projects\\SIG-COG\\ML\\link_state_lstm.h5\",  # <-- your model path\n",
    "    scaler=scaler,\n",
    "    class_order=CLASS_ORDER\n",
    ")\n",
    "outcome_node = OutcomeEvaluatorNode(\n",
    "    model=classifier_node.model,\n",
    "    class_order=CLASS_ORDER,\n",
    "    delta_thresh=0.10,\n",
    "    simulation_mode=SIMULATION_MODE\n",
    ")\n",
    "\n",
    "# ---- action fallback (cold-start / no-memory) ----\n",
    "FALLBACK_ACTIONS = [\n",
    "    \"Lower Modulation to QPSK + Max FEC\",\n",
    "    \"Increase Transmit Power\",\n",
    "    \"Switch Frequency Band\",\n",
    "    \"Apply Beamforming\",\n",
    "    \"Use Relay Node\",\n",
    "]\n",
    "\n",
    "def fallback_decide_action(current_state: str) -> str:\n",
    "    # simple heuristic before memory/LLM exist\n",
    "    if current_state == \"Poor\":\n",
    "        return \"Lower Modulation to QPSK + Max FEC\"\n",
    "    if current_state == \"Moderate\":\n",
    "        return \"Increase Transmit Power\"\n",
    "    return \"Keep current settings\"\n",
    "\n",
    "# ---- graph nodes (wrappers around your classes/tools) ----\n",
    "def classify_node(state: AgentState) -> AgentState:\n",
    "    return classifier_node(state, global_log)\n",
    "\n",
    "def retrieve_node(state: AgentState) -> AgentState:\n",
    "    if state[\"predicted_state\"] == \"Good\":\n",
    "        # skip retrieval entirely for Good\n",
    "        state[\"similar_case\"] = []\n",
    "        state[\"log\"].append(f\"[Step {state['step']}] Good → skipping RAG/LLM.\")\n",
    "        global_log.add_log(state[\"log\"][-1])\n",
    "        return state\n",
    "    retrieved = rag_memory.retrieve(state[\"window_features\"], k=3)\n",
    "    state[\"similar_case\"] = retrieved\n",
    "    msg = f\"[Step {state['step']}] Retrieved {len(retrieved)} similar cases.\"\n",
    "    state[\"log\"].append(msg)\n",
    "    global_log.add_log(msg)\n",
    "    return state\n",
    "\n",
    "def select_action_node(state: AgentState) -> AgentState:\n",
    "    if state[\"predicted_state\"] == \"Good\":\n",
    "        state[\"action\"] = \"Keep current settings\"\n",
    "        return state\n",
    "\n",
    "    # Cold-start or no helpful memory → use fallback (trial-and-error baseline)\n",
    "    if not state[\"similar_case\"]:\n",
    "        action = fallback_decide_action(state[\"predicted_state\"])\n",
    "        state[\"action\"] = action\n",
    "        msg = f\"[Step {state['step']}] No memory yet → fallback action: {action}\"\n",
    "        state[\"log\"].append(msg); global_log.add_log(msg)\n",
    "        return state\n",
    "\n",
    "    # Otherwise ask LLM (still cached)\n",
    "    action = select_action.invoke({\n",
    "        \"current_state\": state[\"predicted_state\"],\n",
    "        \"retrieved_memories\": state[\"similar_case\"]\n",
    "    })\n",
    "    state[\"action\"] = action\n",
    "    msg = f\"[Step {state['step']}] LLM selected action: {action}\"\n",
    "    state[\"log\"].append(msg); global_log.add_log(msg)\n",
    "    return state\n",
    "\n",
    "def execute_node(state: AgentState) -> AgentState:\n",
    "    if state[\"predicted_state\"] == \"Good\":\n",
    "        # We won’t mutate window in Good state\n",
    "        return state\n",
    "    modified_window = execute_action.invoke({\n",
    "        \"current_window\": state[\"window_features\"],\n",
    "        \"action\": state[\"action\"],\n",
    "        \"simulation_mode\": SIMULATION_MODE\n",
    "    })\n",
    "    state[\"window_features\"] = modified_window\n",
    "    msg = f\"[Step {state['step']}] Executed action.\"\n",
    "    state[\"log\"].append(msg); global_log.add_log(msg)\n",
    "    return state\n",
    "\n",
    "def evaluate_node(state: AgentState) -> AgentState:\n",
    "    if state[\"predicted_state\"] == \"Good\":\n",
    "        # Ensure state is consistent & outcome is trivially success\n",
    "        state[\"outcome\"] = \"Success\"\n",
    "        state[\"log\"].append(f\"[Step {state['step']}] Good window → no action; marked Success.\")\n",
    "        global_log.add_log(state[\"log\"][-1])\n",
    "        return state\n",
    "\n",
    "    state = outcome_node(state, state[\"window_features\"], rag_memory, global_log)\n",
    "    # Also store experience in RAG (ensures first Moderate/Poor creates memory)\n",
    "    rag_memory.add(\n",
    "        input_window=state[\"window_features\"],\n",
    "        state=state[\"predicted_state\"],\n",
    "        action=state[\"action\"],\n",
    "        outcome=state[\"outcome\"]\n",
    "    )\n",
    "    return state\n",
    "\n",
    "# ---- build the graph ----\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"classify\", classify_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"select_action\", select_action_node)\n",
    "graph.add_node(\"execute\", execute_node)\n",
    "graph.add_node(\"evaluate\", evaluate_node)\n",
    "\n",
    "graph.add_edge(\"classify\", \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"select_action\")\n",
    "graph.add_edge(\"select_action\", \"execute\")\n",
    "graph.add_edge(\"execute\", \"evaluate\")\n",
    "\n",
    "graph.set_entry_point(\"classify\")\n",
    "app = graph.compile()\n",
    "\n",
    "# ---- runner over your dataset windows ----\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_agent_over_dataset(csv_path: str, iterations: int, start_mode=\"sequential\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X_raw = df[FEATURE_COLS].values\n",
    "\n",
    "    # ✅ Fit scaler on the whole dataset (OK for simulation)\n",
    "    scaler = StandardScaler().fit(X_raw)\n",
    "    X_scaled = scaler.transform(X_raw)\n",
    "\n",
    "    # ---- pick start indices as before ----\n",
    "    max_start = len(X_scaled) - WINDOW_SIZE\n",
    "    if max_start < 0:\n",
    "        raise ValueError(\"Dataset too short for given WINDOW_SIZE.\")\n",
    "    if start_mode == \"random\":\n",
    "        starts = np.random.randint(0, max_start + 1, size=iterations)\n",
    "    else:\n",
    "        hop = max(1, max_start // max(1, iterations))\n",
    "        starts = np.clip(np.arange(0, hop*iterations, hop), 0, max_start)\n",
    "\n",
    "    for i, s in enumerate(starts, 1):\n",
    "        window_scaled = X_scaled[s:s+WINDOW_SIZE]  # <-- use scaled window\n",
    "\n",
    "        state: AgentState = {\n",
    "            \"step\": i,\n",
    "            \"window_features\": window_scaled,  # <-- keep scaled in state\n",
    "            \"predicted_state\": None,\n",
    "            \"prediction_probs\": None,\n",
    "            \"action\": None,\n",
    "            \"outcome\": None,\n",
    "            \"similar_case\": None,\n",
    "            \"log\": []\n",
    "        }\n",
    "        state = app.invoke(state)\n",
    "        print(f\"\\n[ITER {i}] Pred={state['predicted_state']} | Action={state['action']} | Outcome={state['outcome']}\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== Global Log (tail) ===\")\n",
    "    print(\"\\n\".join(global_log.get_full_log().splitlines()[-min(50, len(global_log.logs)):]))\n",
    "\n",
    "\n",
    "\n",
    "run_agent_over_dataset(DATASET_PATH, ITERATIONS, START_MODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f546d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
